---
author: "Abbie Benfield"
date: '2022-04-17'
output: html_document
---

# Introduction

The dream of any emerging new artist is to produce a ‘charting’ song, one that is on repeat on the radio and making waves across the country. Billboard, one of the biggest music and entertainment magazines in the US, tabulates the relatively popular songs in the US and internationally. These can be ranked according to sales, streams, airplay, among other categories. Produce a song that charts for a week or more, and you make a name for yourself in the music industry. Even more so is when the song was in the top 20 of those charts. 

We were curious to know what kind of songs make it into the top 20 of these charts. That said, we analyzed the Spotify Top 200 Charts from 2020 and 2021 dataset for any trends or similarities in the charting songs. We wanted to know if given a set of predictors as is in the dataset, can we predict if a song will make it into the top 20 of the charts or not. 


```{r, message = FALSE}
library(tidyverse)
library(janitor)
library(skimr)
library(corrplot)
library(gbm)
library(e1071)
library(class)
library(tree)

spotify_full <- read_csv("spotify_dataset.csv") %>%
  clean_names() %>%
  mutate(top_20 = ifelse(highest_charting_position <= 20, TRUE, FALSE))

head(spotify_full)
```

# Exploratory Data Analysis

```{r}
skim(spotify_full)
```

The first thing we noticed when looking at the skim output is that there are a small number of rows with missing values. Since there aren't very many of them, we dropped them when creating our cleaned data set. Something else that caught our eye is that there are fewer song IDs than there are rows, which means some songs are repeated since each song has a unique ID number. However, upon further observation of the data, repeat song IDs are different versions of the same song with different charting information, though the variables that describe the songs features are usually the same.  As such, we decided to keep them because sometimes one version was a top 20 song, while the other wasn't. Also, looking at the small histograms created for the numerical variables, we noticed that some were a bit skewed, while a few were more evenly distributed. Therefore, for the procedures that are sensitive to scale, it reaffirmed our decision to scale the data.


```{r}
ggplot(spotify_full, aes(x = highest_charting_position)) +
  geom_freqpoly(bins = 30) +
  labs(x = "Highest Charting Postion", y = "Count", title = "Distribution of Highest Charting Position")
```

For our primary variable, highest charting position (the variable from which the top 20 variable is calculated), we looked at a more detailed frequency plot for variation, and saw that it was mostly even.

```{r}
clean <- spotify_full %>%
  na.omit() %>%
  select(
    -index, -song_id, -weeks_charted, -week_of_highest_charting, -genre,
    -release_date, -chord, -song_name, -artist, -highest_charting_position
  )

skim(clean)
```

To clean the data, we omitted the few rows with missing values, the highest charting position variable because top 20 is derived from it so it will be highly correlated, and the variables that contain characters because, while some of them might contain interesting information, it is beyond the scope of this project to try and incorporate them as there are too many different values to make them factors.

```{r}
corrplot(cor(clean), method = "circle", tl.col = "black", tl.srt = 45, type = "upper")
```
Lastly, we created a correlation plot. There are some variables with higher than expected correlation, but we decided to leave them all in.


# Methodology

To analyze the data, we first have to split it into two, the test and the training datasets. This will make it easier to see how well the model we would have trained on the training dataset is doing in predicting the results of the test dataset. We opted to do a `sample_n` of the first 1000 rows instead of the `frac` one because we wanted the sampled rows to correspond to those of the train/test rows for the outcome variable, which is not guaranteed if we use `frac`. Though not necessary, we chose to scale our data so we can make it generalizable. 

```{r}
set.seed(1)

test <- clean %>%
  sample_n(1000)

train <- clean %>%
  setdiff(test)

scaled_test <- test %>%
  select(-top_20) %>%
  scale() %>%
  data.frame()

test_20 <- test %>%
  select(top_20) %>%
  .$top_20

scaled_train <- train %>%
  select(-top_20) %>%
  scale() %>%
  data.frame()

train_20 <- train %>%
  select(top_20) %>%
  .$top_20
```


We then incoporated different models in trying to understand the data a bit more, starting with Logistic Regression.

### Logistic Regression

```{r}
glm_fit <- glm(top_20 ~ .,
  data = train,
  family = binomial
)

glm_probs <- data.frame(probs = predict(glm_fit,
  newdata = test,
  type = "response"
))

glm_pred <- glm_probs %>%
  mutate(pred = ifelse(probs > .5, TRUE, FALSE))

glm_pred <- cbind(test, glm_pred)

glm_pred %>%
  count(pred, top_20) %>%
  spread(top_20, n, fill = 0)

glm_pred %>%
  summarize(
    score = mean(pred == top_20),
    recip = mean(pred != top_20)
  )
```


Using 13 of the initial 23 predictors, we fit the logistic regression model on the training data and tested its accuracy on the test dataset. What we notice is that the model does pretty well in predicting songs that will make it into the top 20, correctly categorizing the songs 87% of the time. It's worth mentioning though that the model performs better when categorizing songs that didn't make it into the top 20 as compared to those that did, with the former having a test accuracy of 98% while the latter has a poor 28% test accuracy. Hence this is not the best model predicting if a song will make it into the top 20.

### KNN

```{r}
val <- 0
kmax <- 1

for (k in 1:20) {
  set.seed(1)
  knn_pred <- knn(scaled_train, scaled_test, train_20, k)
  tab <- table(knn_pred, test_20)
  if ((tab[2, 2] / (tab[2, 2] + tab[1, 2])) > val) {
    val <- (tab[2, 2] / (tab[2, 2] + tab[1, 2]))
    kmax <- k
  }
}

val
kmax


knn_preds <- knn(scaled_train, scaled_test, train_20, kmax)
mean(knn_preds != test_20)
table(knn_preds, test_20)
```

### Trees

```{r}
tree <- tree(top_20 ~ ., train)

summary(tree)

tree_probs <- predict(tree, test, type = "vector")[,2]
tree_preds <- ifelse(tree_probs > .2, TRUE, FALSE)
table(tree_preds, test$top_20)
mean(tree_preds != test$top_20)
```


```{r}
# Boosting
set.seed(1)
boost_fit <- gbm(top_20 ~., data = train, distribution = "bernoulli", n.trees = 1000, shrinkage = .01)

summary(boost_fit)

boost_probs <- predict(boost_fit, newdata = test, n.trees = 1000, type = "response")
boost_preds <- ifelse(boost_probs > .2, TRUE, FALSE)
table(test$top_20, boost_preds)
mean(boost_preds != test$top_20)
```



### SVM?

```{r}
set.seed(1)

clean <- clean %>% 
  mutate(top_20 = as.factor(top_20))

test <- clean %>%
  sample_n(1000)

train <- clean %>%
  setdiff(test)

svm_fit <- tune(svm, top_20 ~ ., data = train, kernel = "linear", scale = TRUE, ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))

summary(svm_fit)

bestmod <- svm_fit$best.model
summary(bestmod)

svm_pred <- predict(bestmod, test)
table(predicted = svm_pred, true = test$top_20)

```

