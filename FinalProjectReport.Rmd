---
title: "Final Project"
author: "Abbie Benfield, Annah Mutaya"
date: "5/5/2022"
output:
  html_document: default
---

# Introduction

The dream of any emerging new artist is to produce a 'charting' song, one that is on repeat on the radio and making waves across the country. For our project, we decided to analyze the Spotify Top 200 Charts to see whether we could build a model that accurately predicts whether a song makes it into the top 20. The [dataset]((https://www.kaggle.com/datasets/sashankpillai/spotify-top-200-charts-20202021)) we found consists of all 1556 songs found on the Spotify Top 200 Charts in 2020 and 2021, described by 23 different variables.

```{r, message = FALSE}
library(tidyverse)
library(janitor)
library(skimr)
library(corrplot)
library(gbm)
library(e1071)
library(class)

spotify_full <- read_csv("spotify_dataset.csv") %>%
  clean_names() %>%
  mutate(top_20 = ifelse(highest_charting_position <= 20, TRUE, FALSE))

head(spotify_full)
```

# Exploratory Data Analysis

```{r}
skim(spotify_full)
```

The first thing we noticed when looking at the skim output is that there are a small number of rows with missing values. Since there aren't many of them, we dropped them when creating our cleaned data set. Something else that caught our eye is that there are fewer song IDs than rows, which means some songs repeat since each has a unique ID number. However, upon further observation, repeat IDs are separate versions of the song with different charting information, though the variables that describe the songs' features are usually the same. Therefore, we decided to keep them because sometimes one version was a top 20 song while the other wasn't. Also, looking at the small histograms created for the numerical variables, we noticed that some were a bit skewed while others were distributed more evenly. Thus, it reaffirmed our decision to scale the data for the procedures that are sensitive to scale.

```{r}
ggplot(spotify_full, aes(x = highest_charting_position)) +
  geom_freqpoly(bins = 30) +
  labs(x = "Highest Charting Postion", y = "Count", title = "Distribution of Highest Charting Position")
```

For our primary variable, highest charting position (which is how we calculated top_20), we looked at a more detailed frequency plot to observe variation and saw that it was mostly evenly distributed.

```{r}
clean <- spotify_full %>%
  na.omit() %>%
  select(
    -index, -song_id, -weeks_charted, -week_of_highest_charting, -genre,
    -release_date, -chord, -song_name, -artist, -highest_charting_position
  )

skim(clean)
```

To clean the data, we omitted the rows with missing values, the highest charting position variable because top_20 is derived from it, and the variables that contain characters because, while some of them may have interesting information, it is beyond the scope of this project to try and incorporate them as there are too many unique values to make them factors.

```{r}
corrplot(cor(clean), method = "circle", tl.col = "black", tl.srt = 45, type = "upper")
```

Lastly, we created a correlation plot. There are some variables with higher than expected correlation, but we decided to leave them all in because, except for energy and loudness, they weren't strongly correlated. We kept energy and loudness because they weren't highly correlated with other variables.

# Methodology

We first tried to utilize best subset selection, but we could not to run the code to completion.

To analyze the data, we split it into test and training datasets. We opted to do a sample of the first 1000 rows instead of using a fraction because we wanted the sampled rows to correspond to the rows of the outcome variable, which would not be guaranteed if we used a fraction. Though not necessary, we chose to scale our data to make it generalizable.

```{r}
set.seed(1)

test <- clean %>%
  sample_n(1000)

train <- clean %>%
  setdiff(test)

scaled_test <- test %>%
  select(-top_20) %>%
  scale() %>%
  data.frame()

test_20 <- test %>%
  select(top_20) %>%
  .$top_20

scaled_train <- train %>%
  select(-top_20) %>%
  scale() %>%
  data.frame()

train_20 <- train %>%
  select(top_20) %>%
  .$top_20
```

We then tried different models to try and find the one best accuracy, starting with logistic regression.

## Logistic Regression

```{r}
glm_fit <- glm(top_20 ~ .,
  data = train,
  family = binomial
)

glm_probs <- data.frame(probs = predict(glm_fit,
  newdata = test,
  type = "response"
))

glm_pred <- glm_probs %>%
  mutate(pred = ifelse(probs > .5, TRUE, FALSE))

glm_pred <- cbind(test, glm_pred)

glm_pred %>%
  count(pred, top_20) %>%
  spread(top_20, n, fill = 0)

glm_pred %>%
  summarize(
    score = mean(pred == top_20),
    recip = mean(pred != top_20)
  )

45 / (45 + 118)
```

After fitting the logistic regression model, we notice that the model does pretty well in predicting whether a song is in the top 20, incorrectly categorizing only 13.5% of the time. However, it's worth mentioning that the model performs better when classifying songs that didn't make it into the top 20 compared to those that did. The former has a test accuracy of 98%, while the latter has a poor test accuracy of only 28%. Hence this is not the best model for predicting whether a song will make it into the top 20.

## KNN

We next tried KNN.

```{r}
val <- 0
kmax <- 1

for (k in 1:20) {
  set.seed(1)
  knn_pred <- knn(scaled_train, scaled_test, train_20, k)
  tab <- table(knn_pred, test_20)
  if ((tab[2, 2] / (tab[2, 2] + tab[1, 2])) > val) {
    val <- (tab[2, 2] / (tab[2, 2] + tab[1, 2]))
    kmax <- k
  }
}

val
kmax


knn_preds <- knn(scaled_train, scaled_test, train_20, kmax)
mean(knn_preds != test_20)
table(knn_preds, test_20)
```

Using the scaled data, we tried KNN classification to see how well it performed on the test dataset. From a possible 20, we selected the best value of k that gave the lowest testing error rate, which was 1. KNN did better than the logistic model at correctly classifying top 20 songs, though still relatively poorly, at only 35% accurate. Additionally, the overall error rate was worse, at 16.3%.

## Trees

We also tried a tree model with boosting.

```{r}
# Boosting
set.seed(1)
boost_fit <- gbm(top_20 ~ ., data = train, distribution = "bernoulli", n.trees = 1000, shrinkage = .01)

summary(boost_fit)

boost_probs <- predict(boost_fit, newdata = test, n.trees = 1000, type = "response")
boost_preds <- ifelse(boost_probs > .2, TRUE, FALSE)
table(test$top_20, boost_preds)
mean(boost_preds != test$top_20)
116 / (98 + 116)
```

Tree boosting did better than both logistic regression and KNN at accurately predicting top 20 songs, with an accuracy of 54%, though still not the best. Its overall test error rate was between logistic and KNN, at 14.5%

## SVM

The last model we tried was SVM.

```{r}
set.seed(1)

clean <- clean %>%
  mutate(top_20 = as.factor(top_20))

test <- clean %>%
  sample_n(1000)

train <- clean %>%
  setdiff(test)

svm_fit <- tune(svm, top_20 ~ ., data = train, kernel = "linear", scale = TRUE, ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))

summary(svm_fit)

bestmod <- svm_fit$best.model
summary(bestmod)

svm_pred <- predict(bestmod, test)
table(predicted = svm_pred, true = test$top_20)

(14 + 124) / 1000 # Overall error rate
39 / (39 + 124) # Accuracy rate for top 20 classification
```

To try and improve the results from logistic regression, we tried an SVM model with a linear kernel and found the best cost value was 1. We first had to change the outcome variable to a factor. Though the best model output claims the overall test error rate was 11.2%, the manual calculation using the actual predicted values gave a test error rate of 13.8%. Additionally, the accuracy for the songs that made it into the top 20 was even lower, at 24%, so it didn't improve on logistic regression as we had hoped.

Though none of the models did exceptionally well, we choose the tree boosting model as our final model because it had the highest classification accuracy for top 20 songs, even though it had a higher overall error rate than logistic and SVM (see table below).

|                    | Logistic | KNN | Tree | SVM |
|--------------------|----------|-----|------|-----|
| Overall Error Rate | 13%      | 16% | 14%  | 13% |
| Top 20 Accuracy    | 28%      | 35% | 54%  | 24% |

: Model Comparison

# Conclusion

Our final step was to apply our chosen tree boosting model to the complete dataset, first converting the top 20 variable back to a logical type.

```{r}
clean <- clean %>%
  mutate(top_20 = as.logical(top_20))

final_probs <- predict(boost_fit, newdata = clean, n.trees = 1000, type = "response")
final_preds <- ifelse(final_probs > .2, TRUE, FALSE)
table(clean$top_20, final_preds)
mean(final_preds != clean$top_20)
178 / (141 + 178)
```

The results were not significantly different from those obtained from the test data. However, they were slightly better, with an overall error rate of 13.0% and a top 20 accuracy rate of 56%.

Our main takeaway from this project is that there is a likely a better model that could be created for this dataset, given the low accuracy rate for predicting top 20 songs. Future analysis could utilize better feature selection techniques and expand the types of models explored.


# References

- [EDA Inspriation](https://www.kaggle.com/code/ippudkiippude/spotify-dataset-eda-prediction-classification) 
- [Correlation Plots](http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram#:~:text=Visualization%20methods,-Seven%20different%20visualization&text=Positive%20correlations%20are%20displayed%20in,proportional%20to%20the%20correlation%20coefficients)




